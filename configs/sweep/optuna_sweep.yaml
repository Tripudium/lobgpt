# Optuna hyperparameter sweep configuration
# @package _global_

defaults:
  - override /model: lob_transformer_small
  - override /data: btc_usdt
  - override /training: default

# Optuna study configuration
optuna:
  study_name: "lob_transformer_optimization"
  storage: "sqlite:///optuna_study.db"
  direction: "minimize"  # minimize val_loss
  n_trials: 100
  timeout: null  # No timeout

  # Pruning
  pruner:
    _target_: optuna.pruners.MedianPruner
    n_startup_trials: 5
    n_warmup_steps: 30
    interval_steps: 10

  # Sampler
  sampler:
    _target_: optuna.samplers.TPESampler
    seed: 42

# Override training for sweep
training:
  max_epochs: 30  # Shorter for grid search

# Parameter search space
search_space:
  # Model architecture
  model.d_model:
    type: categorical
    choices: [256, 512, 768]

  model.n_layers:
    type: categorical
    choices: [4, 6, 8, 12]

  model.n_heads:
    type: categorical
    choices: [4, 8, 12, 16]

  model.dropout:
    type: uniform
    low: 0.05
    high: 0.3

  # Training hyperparameters
  training.learning_rate:
    type: loguniform
    low: 1e-5
    high: 1e-3

  training.weight_decay:
    type: loguniform
    low: 1e-4
    high: 1e-1

  training.warmup_steps:
    type: int
    low: 500
    high: 5000
    step: 500

  # Data parameters
  data.batch_size:
    type: categorical
    choices: [8, 16, 32, 64]

  data.sequence_length:
    type: categorical
    choices: [128, 200, 300, 512]

  # Optimizer parameters
  training.optimizer.betas:
    type: categorical
    choices: [[0.9, 0.95], [0.9, 0.999], [0.95, 0.999]]

# Fixed parameters for consistency
model:
  d_ff: null  # Will be set to 4 * d_model
  tie_embeddings: true
  use_time_encoding: true

data:
  horizon: 10
  threshold: 0.002