# Default training configuration

# Training parameters
max_epochs: 50
learning_rate: 3e-4
weight_decay: 0.01
warmup_steps: 2000
scheduler_type: "cosine"  # cosine, onecycle

# Optimization
gradient_clip_val: 1.0
accumulate_grad_batches: 1

# Adam optimizer
optimizer:
  betas: [0.9, 0.95]
  eps: 1e-8

# Learning rate scheduler
scheduler:
  # For cosine annealing
  eta_min_factor: 0.1

  # For OneCycle
  pct_start: 0.1
  div_factor: 25.0
  final_div_factor: 10000.0